% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deepregression.R
\name{deepregression}
\alias{deepregression}
\title{Fitting Deep Distributional Regression}
\usage{
deepregression(
  y,
  list_of_formulae,
  list_of_deep_models,
  family = c("normal", "bernoulli", "bernoulli_prob", "beta", "betar", "cauchy",
    "chi2", "chi", "exponential", "gamma_gamma", "gamma", "gammar", "gumbel",
    "half_cauchy", "half_normal", "horseshoe", "inverse_gamma", "inverse_gaussian",
    "laplace", "log_normal", "logistic", "multinomial", "multinoulli", "negbinom",
    "pareto", "poisson", "poisson_lograte", "student_t", "student_t_ls",
    "truncated_normal", "uniform", "zip"),
  train_together = FALSE,
  data,
  df = NULL,
  lambda_lasso = NULL,
  lambda_ridge = NULL,
  defaultSmoothing = NULL,
  cv_folds = NULL,
  validation_data = NULL,
  validation_split = ifelse(is.null(validation_data) & is.null(cv_folds), 0.2, 0),
  dist_fun = NULL,
  learning_rate = 0.01,
  optimizer = optimizer_adam(lr = learning_rate),
  variational = FALSE,
  monitor_metric = list(),
  seed = 1991 - 5 - 4,
  mixture_dist = 0,
  split_fun = split_model,
  posterior_fun = posterior_mean_field,
  prior_fun = prior_trainable,
  null_space_penalty = variational,
  ind_fun = function(x) tfd_independent(x),
  extend_output_dim = 0,
  offset = NULL,
  offset_val = NULL,
  absorb_cons = TRUE,
  zero_constraint_for_smooths = FALSE,
  ...
)
}
\arguments{
\item{y}{response variable}

\item{list_of_formulae}{a named list of right hand side formulae,
one for each parameter of the distribution specified in \code{family};
set to \code{~ 1} if the parameter should be treated as constant.
Use the \code{s()}-notation from \code{mgcv} for specification of
non-linear structured effects and \code{d(...)} for
deep learning predictors (predictors in brackets are separated by commas),
where \code{d} can be replaced by an name name of the names in 
\code{list_of_deep_models}, e.g., \code{~ 1 + s(x) + my_deep_mod(a,b,c)},
where my_deep_mod is the name of the neural net specified in 
\code{list_of_deep_models} and \code{a,b,c} are features modeled via
this network.}

\item{list_of_deep_models}{a named list of functions
specifying a keras model.
See the examples for more details.}

\item{family}{a character specifying the distribution. For information on 
possible distribution and parameters, see \code{\link{make_tfd_dist}}}

\item{train_together}{logical; whether or not to train all parameters in 
one deep network.}

\item{data}{data.frame or named list with input features}

\item{df}{degrees of freedom for all non-linear structural terms}

\item{lambda_lasso}{smoothing parameter for lasso regression; 
can be combined with ridge}

\item{lambda_ridge}{smoothing parameter for ridge regression; 
can be combined with lasso}

\item{defaultSmoothing}{function applied to all s-terms, per default (NULL)
the minimum df of all possible terms is used.}

\item{cv_folds}{a list of lists, each list element has two elements, one for
training indices and one for testing indices; 
if a single integer number is given, 
a simple k-fold cross-validation is defined, where k is the supplied number.}

\item{validation_data}{data for validation during training.}

\item{validation_split}{percentage of training data used for validation. 
Per default 0.2.}

\item{dist_fun}{a custom distribution applied to the last layer,
see \code{\link{make_tfd_dist}} for more details on how to construct
a custom distribution function.}

\item{learning_rate}{learning rate for optimizer}

\item{optimizer}{optimzer used. Per default ADAM.}

\item{variational}{logical value specifying whether or not to use
variational inference. If \code{TRUE}, details must be passed to
the via the ellipsis to the initialization function
(see \code{\link{deepregression_init}})}

\item{monitor_metric}{Further metrics to monitor}

\item{seed}{integer value used as a seed in data splitting}

\item{mixture_dist}{integer either 0 or >= 2. If 0 (default), no mixture distribution is fitted.
If >= 2, a network is constructed that outputs a multivariate response for each of the mixture 
components.}

\item{split_fun}{a function separating the deep neural network in two parts
so that the orthogonalization can be applied to the first part before 
applying the second network part; per default, the function \code{split_model} is
used which assumes a dense layer as penultimate layer and separates the network
into a first part without this last layer and a second part only consisting of a 
single dense layer that is fed into the output layer}

\item{posterior_fun}{function defining the posterior function for the variational
verison of the network}

\item{prior_fun}{function defining the prior function for the variational
verison of the network}

\item{null_space_penalty}{logical value;
if TRUE, the null space will also be penalized for smooth effects. 
Per default, this is equal to the value give in \code{variational}.}

\item{ind_fun}{function applied to the model output before calculating the 
log-likelihood. Per default independence is assumed by applying \code{tfd_independent}.}

\item{extend_output_dim}{integer value >= 0 for extending the output dimension by an 
additive constant. If set to a value > 0, a multivariate response with dimension
\code{1 + extend_output_dim} is defined.}

\item{offset}{a list of column vectors (i.e. matrix with ncol = 1) or NULLs for each parameter,
in case an offset should be added to the additive predictor; if NULL, no offset is used}

\item{offset_val}{a list analogous to offset for the validation data}

\item{absorb_cons}{logical; adds identifiability constraint to the basisi. 
See \code{?mgcv::smoothCon} for more details.}

\item{zero_constraint_for_smooths}{logical; the same as absorb_cons, 
but done explicitly. If true a constraint is put on each smooth to have zero mean.}

\item{...}{further arguments passed to the \code{deepregression\_init} function}
}
\description{
Fitting Deep Distributional Regression
}
\examples{
library(deepregression)

data = data.frame(matrix(rnorm(10*100), c(100,10)))
colnames(data) <- c("x1","x2","x3","xa","xb","xc","xd","xe","xf","unused")
formula <- ~ 1 + deep_model(x1,x2,x3) +
s(xa, sp = 1) + x1

deep_model <- function(x) x \%>\%
layer_dense(units = 32, activation = "relu", use_bias = FALSE) \%>\%
layer_dropout(rate = 0.2) \%>\%
layer_dense(units = 8, activation = "relu") \%>\%
layer_dense(units = 1, activation = "linear")

y <- rnorm(100)

mod <- deepregression(list_of_formulae = list(loc = formula, scale = ~ 1),
data = data, validation_data = list(data, y), y = y,
list_of_deep_models = list(deep_model = deep_model))

mod \%>\% fit(epochs = 100)
mod \%>\% plot()

}
